# Resume Parser Edge Function

AI-powered resume parser backend using OpenAI gpt-4o-mini.

## Overview

This Supabase Edge Function converts PDF/DOCX resume files into structured YAML ready for import into the Resume Builder app.

**Features:**
- ‚úÖ Parses PDF and DOCX files
- ‚úÖ Extracts contact info, experience, education, skills, and more
- ‚úÖ Returns YAML format matching app templates
- ‚úÖ Hash-based caching (60%+ cache hit rate)
- ‚úÖ Guard rails against non-resume files
- ‚úÖ Prompt injection prevention
- ‚úÖ Schema validation

**Cost:** ~$0.002 per upload with caching (~$4/month for 1,000 users)

---

## Prerequisites

1. **Supabase CLI** (install if not already):
   ```bash
   npm install -g supabase
   ```

2. **OpenAI API Key**:
   - Get key from: https://platform.openai.com/api-keys
   - Should start with `sk-`

3. **Supabase Project Keys**:
   - URL: `https://mgetvioaymkvafczmhwo.supabase.co`
   - Anon Key: (Dashboard > Settings > API > `anon` / `public`)
   - **Service Role Key**: (Dashboard > Settings > API > `service_role`)
     - ‚ö†Ô∏è **Required for global cache deduplication**
     - Never expose to frontend - server-side only

---

## Deployment Steps

### 1. Deploy Database Migration

Run the migration to create the `parsed_resumes` table:

```bash
cd C:\projects\resume-builder

# Login to Supabase (if not already)
supabase login

# Link to your project
supabase link --project-ref mgetvioaymkvafczmhwo

# Push migration
supabase db push
```

**Verify migration:**
```bash
# Check if parsed_resumes table exists
supabase db ls
```

### 2. Set Environment Variables

Set the required secrets:

```bash
# Set OpenAI API key (required - not auto-injected)
supabase secrets set OPENAI_API_KEY=sk-your-api-key-here

# Verify secrets
supabase secrets list
```

**Expected output:**
```
OPENAI_API_KEY
```

**Note**: `SUPABASE_SERVICE_ROLE_KEY` is **automatically available** in Edge Functions and won't appear in the secrets list. Supabase auto-injects this along with:
- `SUPABASE_URL` - Your project URL
- `SUPABASE_ANON_KEY` - Your anon/public key
- `SUPABASE_DB_URL` - Direct database connection

No manual configuration needed for these! üéâ

**‚ö†Ô∏è Security Note:**
- `SUPABASE_SERVICE_ROLE_KEY` is auto-injected by Supabase with admin privileges
- It bypasses Row Level Security (RLS)
- Used ONLY for:
  - Verifying user JWTs (line 88)
  - Global cache operations on `parsed_resumes` table
- Never exposed to frontend - exists only in edge function server environment
- The function still requires user JWT authentication before allowing any operations

### 3. Deploy Edge Function

Deploy the parse-resume function:

```bash
# Deploy function
supabase functions deploy parse-resume

# Verify deployment
supabase functions list
```

**Expected output:**
```
Deployed Functions:
  ‚îú‚îÄ parse-resume (20XX-XX-XX XX:XX:XX)
```

### 4. Test the Function

Test with curl:

```bash
# Get your JWT token from Supabase Dashboard or login
# Replace YOUR_JWT_TOKEN with actual token

curl -X POST https://mgetvioaymkvafczmhwo.supabase.co/functions/v1/parse-resume \
  -H "Authorization: Bearer YOUR_JWT_TOKEN" \
  -F "file=@path/to/resume.pdf"
```

**Expected response:**
```json
{
  "success": true,
  "yaml": "template: modern\nfont: Arial\n...",
  "confidence": 0.95,
  "warnings": [],
  "cached": false
}
```

---

## API Documentation

### Endpoint

```
POST https://mgetvioaymkvafczmhwo.supabase.co/functions/v1/parse-resume
```

### Request

**Headers:**
```
Authorization: Bearer <supabase_jwt_token>
Content-Type: multipart/form-data
```

**Body:**
```
file: <File> (PDF or DOCX, max 10MB)
```

### Response

**Success (200):**
```json
{
  "success": true,
  "yaml": "template: modern\nfont: Arial\ncontact_info:\n  name: John Doe\n...",
  "confidence": 0.95,
  "warnings": ["Phone number not found, used placeholder"],
  "cached": false,
  "file_info": {
    "name": "resume.pdf",
    "size": 123456,
    "type": "pdf"
  }
}
```

**Error (400/401/500):**
```json
{
  "success": false,
  "error": "Error message here",
  "validation_errors": ["Field X missing"],
  "raw_text": "Extracted text for debugging..."
}
```

### Error Codes

- **400**: Invalid file type, file too large, not a resume, low confidence
- **401**: Missing or invalid JWT token
- **500**: Text extraction failed, AI parsing failed, internal error

---

## Architecture

```
User uploads PDF/DOCX
       ‚Üì
Edge Function: /parse-resume
       ‚Üì
1. Authenticate user (JWT with anon key client)
2. Calculate file hash (SHA-256)
3. Check global cache (service role client) ‚Üí Return if cached
4. Validate file (type, size, magic numbers)
5. Extract text (PDF/DOCX)
6. Guard rail: Is this a resume?
7. Call OpenAI gpt-4o-mini
8. Validate schema
9. Convert to YAML
10. Cache in DB globally (service role client, 30-day TTL)
11. Return YAML
```

### Security Model: Dual-Client Architecture

The function uses **two separate Supabase clients** for security:

1. **User Client** (`anon key` + JWT):
   - Used for: Authentication only (step 1)
   - Respects: Row Level Security (RLS)
   - Access: User can only see their own data

2. **Admin Client** (`service_role key`):
   - Used for: Cache operations only (steps 3, 10)
   - Bypasses: RLS for global deduplication
   - Access: Can read/write all cached resumes

**Why this approach?**
- ‚úÖ **User privacy**: End users cannot query other users' PII via RLS
- ‚úÖ **Cost efficiency**: Same resume uploaded by different users = cache hit
- ‚úÖ **Security**: Service role key never exposed to frontend
- ‚úÖ **Deduplication**: Hash-based cache works globally across all users

---

## File Structure

```
supabase/functions/parse-resume/
‚îú‚îÄ‚îÄ index.ts                      # Main entry point
‚îú‚îÄ‚îÄ types.ts                      # TypeScript types
‚îú‚îÄ‚îÄ deno.json                     # Dependencies
‚îú‚îÄ‚îÄ README.md                     # This file
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ hash.ts                   # SHA-256 hashing
‚îÇ   ‚îú‚îÄ‚îÄ resume-detector.ts        # Guard rail (keyword check)
‚îÇ   ‚îú‚îÄ‚îÄ yaml-converter.ts         # JSON ‚Üí YAML
‚îÇ   ‚îî‚îÄ‚îÄ file-validator.ts         # File validation
‚îú‚îÄ‚îÄ extractors/
‚îÇ   ‚îú‚îÄ‚îÄ pdf-extractor.ts          # PDF text extraction
‚îÇ   ‚îî‚îÄ‚îÄ docx-extractor.ts         # DOCX text extraction
‚îî‚îÄ‚îÄ ai/
    ‚îú‚îÄ‚îÄ prompts.ts                # OpenAI system prompt
    ‚îú‚îÄ‚îÄ openai-client.ts          # OpenAI API wrapper
    ‚îî‚îÄ‚îÄ validator.ts              # Schema validation
```

---

## Monitoring

### View Logs

```bash
# Real-time logs
supabase functions logs parse-resume --follow

# Recent logs
supabase functions logs parse-resume --limit 50
```

### Metrics to Track

1. **Cache Hit Rate**: Should be >50%
   - Query: `SELECT COUNT(*) FROM parsed_resumes WHERE created_at > NOW() - INTERVAL '24 hours'`

2. **Average Confidence**: Should be >0.80
   - Query: `SELECT AVG(confidence_score) FROM parsed_resumes`

3. **OpenAI Costs**: Check OpenAI dashboard
   - Target: <$10/month for 1,000 users

4. **Error Rate**: Monitor logs for 500 errors
   - Target: <5%

---

## Troubleshooting

### Issue: "Missing Authorization header"
**Solution:** Ensure JWT token is passed in `Authorization: Bearer <token>` header

### Issue: "File does not appear to be a resume"
**Solution:**
- Upload must contain resume keywords (experience, education, skills, etc.)
- Try a different resume file
- Check if PDF is scanned (use text-based PDF, not image)

### Issue: "Low confidence: File may not be a valid resume"
**Solution:**
- AI confidence <0.60 indicates unclear structure
- Try a simpler resume format
- Ensure resume has clear sections (Experience, Education, etc.)

### Issue: "OpenAI API error"
**Solution:**
- Check OpenAI API key is correct: `supabase secrets list`
- Verify OpenAI account has credits
- Check OpenAI status: https://status.openai.com/

### Issue: "Parsing failed validation"
**Solution:**
- Check logs: `supabase functions logs parse-resume`
- Contact support with `validation_errors` from response

---

## Development

### Local Testing (if Supabase CLI supports it)

```bash
# Serve locally
supabase functions serve parse-resume --env-file supabase/.env

# Test locally
curl -X POST http://localhost:54321/functions/v1/parse-resume \
  -H "Authorization: Bearer YOUR_LOCAL_TOKEN" \
  -F "file=@test.pdf"
```

### Update Function

After making code changes:

```bash
# Redeploy
supabase functions deploy parse-resume

# Verify
supabase functions logs parse-resume --limit 10
```

---

## Security

- ‚úÖ **JWT authentication required** - Users must be authenticated
- ‚úÖ **File size limit (10MB)** - Prevents abuse
- ‚úÖ **File type validation (magic numbers)** - Prevents malicious files
- ‚úÖ **Resume keyword detection (guard rail)** - Rejects non-resume files
- ‚úÖ **Prompt injection prevention** - Text sanitization
- ‚úÖ **Text truncation (50K chars max)** - Cost control
- ‚úÖ **Schema validation** - Ensures output quality
- ‚úÖ **RLS policies** - Users cannot query other users' PII directly
- ‚úÖ **Dual-client architecture** - Service role used only for cache (trusted code)
- ‚úÖ **Cache expiry (30 days)** - Auto-cleanup of old data

### PII Protection Model

**The Problem:**
- The `parsed_resumes` table contains full resume text and contact info (PII)
- If we allowed `SELECT USING (true)` in RLS, any user could query other users' data

**The Solution:**
- **RLS Policy**: Restrictive - users can only SELECT their own records
- **Service Role**: Function uses service role key to bypass RLS for cache operations
- **Trust Boundary**: Service role key is server-side only, never exposed to frontend
- **Result**: Global cache deduplication without compromising user privacy

---

## Cost Optimization

**Global Caching Strategy:**
- File hash (SHA-256) used as cache key
- Same file uploaded by **any user** = cache hit (via service role)
- 30-day TTL = balance between cost savings and freshness
- Service role enables global deduplication while maintaining RLS privacy

**Expected Savings:**
- Without cache: 2,000 parses/month √ó $0.005 = $10/month
- With 60% cache hit: 800 parses/month √ó $0.005 = $4/month
- **Savings: 60% ($6/month)**

**Note:** Global cache (vs user-scoped) increases hit rate significantly since popular resumes (e.g., from career services) get reused across users.

---

## Next Steps

1. **Frontend Integration** - Add "Upload Resume" button to UI
2. **Testing** - Beta test with real resume files
3. **Prompt Refinement** - Improve accuracy based on failures
4. **Icon Suggestions** - AI suggests icons for companies/schools (Phase 2)
5. **Grammar Checking** - Inline AI suggestions in editor (Phase 3)

---

## Support

- **GitHub Issues**: https://github.com/anthropics/resume-builder/issues
- **Supabase Logs**: `supabase functions logs parse-resume`
- **OpenAI Status**: https://status.openai.com/
